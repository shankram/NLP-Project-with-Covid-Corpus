{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6ee368eb",
      "metadata": {
        "id": "6ee368eb"
      },
      "source": [
        "IMPORTING REQUIRED PACKAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b6c3f9b3",
      "metadata": {
        "id": "b6c3f9b3"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import logging \n",
        "from tqdm import tqdm \n",
        "import re\n",
        "import scipy\n",
        "import torch\n",
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors, word2vec # Word2Vec model\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine(a, b):\n",
        "    return 1-scipy.spatial.distance.cosine(a,b)"
      ],
      "metadata": {
        "id": "lfhhsfNCz01Y"
      },
      "id": "lfhhsfNCz01Y",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5ZdZ1LDArVW",
        "outputId": "74780c62-74b8-4e9c-f572-6a8ae5c5b012"
      },
      "id": "J5ZdZ1LDArVW",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8OtavOCBPPp",
        "outputId": "d3bfa152-a799-4fe5-d2a5-9a261a575b0c"
      },
      "id": "f8OtavOCBPPp",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe19070",
      "metadata": {
        "id": "3fe19070"
      },
      "source": [
        "EXTRACTING THE FIRST 1000 FILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8b1bf68b",
      "metadata": {
        "id": "8b1bf68b",
        "outputId": "6b696b30-5942-4ca2-9434-794e2fbfe617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79385/79385 [00:00<00:00, 1260810.12it/s]\n"
          ]
        }
      ],
      "source": [
        "corpus = open('/content/drive/MyDrive/dataset/sentences.txt', 'r', encoding=\"utf-8\")\n",
        "buffer = corpus.read()\n",
        "corpus.close()\n",
        "\n",
        "count = 0\n",
        "wr = 0\n",
        "for i in range(len(buffer)):\n",
        "    if buffer[i]=='\\n':\n",
        "        count+=1\n",
        "        if count==1000:\n",
        "            wr = i\n",
        "            break\n",
        "sentences1000 = buffer[:wr+1]\n",
        "sentences1000 = re.sub('\\n', '', sentences1000)\n",
        "\n",
        "sentences = sent_tokenize(sentences1000)\n",
        "sl = []\n",
        "for s in tqdm(sentences):\n",
        "    if (len(s)>15) and (len(s)<=200):\n",
        "        sl.append(s)\n",
        "buffer = '\\n'.join(sl)\n",
        "\n",
        "tokens = re.sub('\\n', '', buffer)\n",
        "tokens = re.split(' ', tokens)\n",
        "\n",
        "f = open('sentencecorpus.txt', 'w', encoding='utf-8')\n",
        "f.write(buffer)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38f99024",
      "metadata": {
        "id": "38f99024"
      },
      "source": [
        "TRAINING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7e4e2767",
      "metadata": {
        "id": "7e4e2767",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8265faf5-636a-43a7-de7c-bbba188d18f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-25 21:02:53,076 : INFO : collecting all words and their counts\n",
            "2022-05-25 21:02:53,140 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2022-05-25 21:02:53,286 : INFO : PROGRESS: at sentence #10000, processed 198671 words, keeping 15074 word types\n",
            "2022-05-25 21:02:53,474 : INFO : PROGRESS: at sentence #20000, processed 404931 words, keeping 22224 word types\n",
            "2022-05-25 21:02:53,737 : INFO : PROGRESS: at sentence #30000, processed 607258 words, keeping 27640 word types\n",
            "2022-05-25 21:02:54,040 : INFO : PROGRESS: at sentence #40000, processed 805429 words, keeping 34517 word types\n",
            "2022-05-25 21:02:54,352 : INFO : PROGRESS: at sentence #50000, processed 1006498 words, keeping 39061 word types\n",
            "2022-05-25 21:02:54,681 : INFO : collected 41738 word types from a corpus of 1161033 raw words and 57692 sentences\n",
            "2022-05-25 21:02:54,695 : INFO : Loading a fresh vocabulary\n",
            "2022-05-25 21:02:55,111 : INFO : effective_min_count=1 retains 41738 unique words (100% of original 41738, drops 0)\n",
            "2022-05-25 21:02:55,118 : INFO : effective_min_count=1 leaves 1161033 word corpus (100% of original 1161033, drops 0)\n",
            "2022-05-25 21:02:55,251 : INFO : deleting the raw counts dictionary of 41738 items\n",
            "2022-05-25 21:02:55,258 : INFO : sample=0.001 downsamples 32 most-common words\n",
            "2022-05-25 21:02:55,260 : INFO : downsampling leaves estimated 869866 word corpus (74.9% of prior 1161033)\n",
            "2022-05-25 21:02:55,446 : INFO : estimated required memory for 41738 words and 100 dimensions: 54259400 bytes\n",
            "2022-05-25 21:02:55,449 : INFO : resetting layer weights\n",
            "2022-05-25 21:03:03,247 : INFO : training model with 8 workers on 41738 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2022-05-25 21:03:04,390 : INFO : EPOCH 1 - PROGRESS: at 41.99% examples, 320130 words/s, in_qsize 12, out_qsize 0\n",
            "2022-05-25 21:03:05,438 : INFO : EPOCH 1 - PROGRESS: at 87.71% examples, 349353 words/s, in_qsize 2, out_qsize 5\n",
            "2022-05-25 21:03:05,489 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2022-05-25 21:03:05,559 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2022-05-25 21:03:05,563 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2022-05-25 21:03:05,565 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2022-05-25 21:03:05,567 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-05-25 21:03:05,581 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-05-25 21:03:05,583 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-25 21:03:05,598 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-25 21:03:05,600 : INFO : EPOCH - 1 : training on 1161033 raw words (869655 effective words) took 2.3s, 370572 effective words/s\n",
            "2022-05-25 21:03:06,652 : INFO : EPOCH 2 - PROGRESS: at 43.64% examples, 367291 words/s, in_qsize 8, out_qsize 3\n",
            "2022-05-25 21:03:07,610 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2022-05-25 21:03:07,623 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2022-05-25 21:03:07,631 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2022-05-25 21:03:07,671 : INFO : EPOCH 2 - PROGRESS: at 96.54% examples, 409716 words/s, in_qsize 4, out_qsize 1\n",
            "2022-05-25 21:03:07,676 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2022-05-25 21:03:07,704 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-05-25 21:03:07,707 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-05-25 21:03:07,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-25 21:03:07,728 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-25 21:03:07,732 : INFO : EPOCH - 2 : training on 1161033 raw words (870337 effective words) took 2.1s, 412136 effective words/s\n",
            "2022-05-25 21:03:08,761 : INFO : EPOCH 3 - PROGRESS: at 41.12% examples, 351557 words/s, in_qsize 14, out_qsize 0\n",
            "2022-05-25 21:03:09,821 : INFO : EPOCH 3 - PROGRESS: at 87.79% examples, 368156 words/s, in_qsize 15, out_qsize 0\n",
            "2022-05-25 21:03:09,895 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2022-05-25 21:03:09,915 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2022-05-25 21:03:09,944 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2022-05-25 21:03:09,985 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2022-05-25 21:03:09,989 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-05-25 21:03:09,991 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-05-25 21:03:10,000 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-25 21:03:10,004 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-25 21:03:10,007 : INFO : EPOCH - 3 : training on 1161033 raw words (870408 effective words) took 2.3s, 384811 effective words/s\n",
            "2022-05-25 21:03:11,025 : INFO : EPOCH 4 - PROGRESS: at 35.93% examples, 310245 words/s, in_qsize 1, out_qsize 0\n",
            "2022-05-25 21:03:12,028 : INFO : EPOCH 4 - PROGRESS: at 77.33% examples, 335381 words/s, in_qsize 12, out_qsize 0\n",
            "2022-05-25 21:03:12,374 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2022-05-25 21:03:12,395 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2022-05-25 21:03:12,408 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2022-05-25 21:03:12,438 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2022-05-25 21:03:12,441 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-05-25 21:03:12,464 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-05-25 21:03:12,471 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-25 21:03:12,473 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-25 21:03:12,475 : INFO : EPOCH - 4 : training on 1161033 raw words (869924 effective words) took 2.5s, 353901 effective words/s\n",
            "2022-05-25 21:03:13,498 : INFO : EPOCH 5 - PROGRESS: at 37.59% examples, 323722 words/s, in_qsize 14, out_qsize 0\n",
            "2022-05-25 21:03:14,519 : INFO : EPOCH 5 - PROGRESS: at 80.70% examples, 346348 words/s, in_qsize 1, out_qsize 0\n",
            "2022-05-25 21:03:14,801 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2022-05-25 21:03:14,852 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2022-05-25 21:03:14,871 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2022-05-25 21:03:14,913 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2022-05-25 21:03:14,916 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-05-25 21:03:14,923 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-05-25 21:03:14,927 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-25 21:03:14,929 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-25 21:03:14,933 : INFO : EPOCH - 5 : training on 1161033 raw words (869763 effective words) took 2.4s, 355462 effective words/s\n",
            "2022-05-25 21:03:15,998 : INFO : EPOCH 6 - PROGRESS: at 34.21% examples, 286510 words/s, in_qsize 0, out_qsize 1\n",
            "2022-05-25 21:03:17,008 : INFO : EPOCH 6 - PROGRESS: at 79.90% examples, 340136 words/s, in_qsize 6, out_qsize 2\n",
            "2022-05-25 21:03:17,289 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2022-05-25 21:03:17,307 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2022-05-25 21:03:17,312 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2022-05-25 21:03:17,317 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2022-05-25 21:03:17,329 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-05-25 21:03:17,336 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-05-25 21:03:17,346 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-25 21:03:17,357 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-25 21:03:17,359 : INFO : EPOCH - 6 : training on 1161033 raw words (870574 effective words) took 2.4s, 362451 effective words/s\n",
            "2022-05-25 21:03:18,404 : INFO : EPOCH 7 - PROGRESS: at 35.93% examples, 305037 words/s, in_qsize 0, out_qsize 0\n",
            "2022-05-25 21:03:19,411 : INFO : EPOCH 7 - PROGRESS: at 75.61% examples, 324390 words/s, in_qsize 8, out_qsize 2\n",
            "2022-05-25 21:03:19,754 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2022-05-25 21:03:19,802 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2022-05-25 21:03:19,808 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2022-05-25 21:03:19,821 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2022-05-25 21:03:19,850 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-05-25 21:03:19,856 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-05-25 21:03:19,866 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-25 21:03:19,883 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-25 21:03:19,889 : INFO : EPOCH - 7 : training on 1161033 raw words (869607 effective words) took 2.5s, 346586 effective words/s\n",
            "2022-05-25 21:03:20,923 : INFO : EPOCH 8 - PROGRESS: at 41.12% examples, 348205 words/s, in_qsize 13, out_qsize 1\n",
            "2022-05-25 21:03:22,005 : INFO : EPOCH 8 - PROGRESS: at 86.78% examples, 360568 words/s, in_qsize 15, out_qsize 2\n",
            "2022-05-25 21:03:22,129 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2022-05-25 21:03:22,138 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2022-05-25 21:03:22,162 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2022-05-25 21:03:22,171 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2022-05-25 21:03:22,185 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-05-25 21:03:22,192 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-05-25 21:03:22,194 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-25 21:03:22,210 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-25 21:03:22,212 : INFO : EPOCH - 8 : training on 1161033 raw words (869756 effective words) took 2.3s, 376032 effective words/s\n",
            "2022-05-25 21:03:23,303 : INFO : EPOCH 9 - PROGRESS: at 41.99% examples, 339890 words/s, in_qsize 10, out_qsize 2\n",
            "2022-05-25 21:03:24,327 : INFO : EPOCH 9 - PROGRESS: at 83.32% examples, 346520 words/s, in_qsize 9, out_qsize 0\n",
            "2022-05-25 21:03:24,581 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2022-05-25 21:03:24,604 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2022-05-25 21:03:24,606 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2022-05-25 21:03:24,618 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2022-05-25 21:03:24,633 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-05-25 21:03:24,642 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-05-25 21:03:24,656 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-25 21:03:24,668 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-25 21:03:24,670 : INFO : EPOCH - 9 : training on 1161033 raw words (869736 effective words) took 2.4s, 356636 effective words/s\n",
            "2022-05-25 21:03:25,697 : INFO : EPOCH 10 - PROGRESS: at 42.88% examples, 368623 words/s, in_qsize 0, out_qsize 1\n",
            "2022-05-25 21:03:26,763 : INFO : EPOCH 10 - PROGRESS: at 85.87% examples, 359410 words/s, in_qsize 15, out_qsize 0\n",
            "2022-05-25 21:03:26,886 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2022-05-25 21:03:26,910 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2022-05-25 21:03:26,925 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2022-05-25 21:03:26,929 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2022-05-25 21:03:26,944 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-05-25 21:03:26,977 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-05-25 21:03:26,979 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-25 21:03:26,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-25 21:03:26,982 : INFO : EPOCH - 10 : training on 1161033 raw words (869905 effective words) took 2.3s, 377770 effective words/s\n",
            "2022-05-25 21:03:26,984 : INFO : training on a 11610330 raw words (8699665 effective words) took 23.7s, 366544 effective words/s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8699665, 11610330)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "sent = open('sentencecorpus.txt', 'r', encoding=\"utf-8\")\n",
        "sents = word2vec.LineSentence(sent)\n",
        "model = word2vec.Word2Vec(size = 100, window = 5, workers = 8, min_count = 1)\n",
        "model.build_vocab(sents)\n",
        "model.train(sents, total_examples = model.corpus_count, epochs = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "klO1EEUDfHj-",
      "metadata": {
        "id": "klO1EEUDfHj-",
        "outputId": "0087f93a-7652-46b2-f66e-d7397f15f553",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "57692it [03:30, 274.59it/s]\n"
          ]
        }
      ],
      "source": [
        "wordvecsize = 100\n",
        "sentvecsize = 125\n",
        "mat = np.zeros((len(sl), sentvecsize))\n",
        "\n",
        "W = np.random.normal(0, 0.01, (sentvecsize, wordvecsize))\n",
        "U = np.random.normal(0, 0.01, (sentvecsize, sentvecsize))\n",
        "h = np.zeros(sentvecsize)\n",
        "for n, sent in tqdm(enumerate(sl)):\n",
        "    for word in re.split(' ', sent.strip()):\n",
        "        try:\n",
        "            x = model.wv[word]\n",
        "            x = scipy.stats.zscore(x)\n",
        "            h = np.tanh((U@h.T) + (W@x.T))\n",
        "        except:\n",
        "            None\n",
        "        mat[n] += h\n",
        "        h = h/sentvecsize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7035a3c2",
      "metadata": {
        "id": "7035a3c2",
        "outputId": "47afa3c2-64ed-4648-f5ac-7ab62dc253ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar sentences to: \n",
            " the surgeon of covid dedicated hospital do rarely practice surgery . \n",
            "\n",
            "some of these guideline include triaging of the patient , prioritizing emergency surgery and delaying the elective surgical procedure till the covid pandemic is over . \n",
            "\n",
            "the society of critical care medicine sccm guideline recommend only that icu admission criterion should select patient who are likely to benefit from icu care . \n",
            "\n",
            "this focus on medical and surgical presentation to the emergency department ed , but fails to provide guidance on triaging psychiatric presentation . \n",
            "\n",
            "the who also produced the pocket book of hospital care for child , providing clinical guideline for physician and nurse delivering pediatric hospital care in resource - limited setting . \n",
            "\n",
            "we acknowledge the ongoing dedicated work of all our clinical colleague , including our hospital palliative care team , intensive care staff and elderly care team . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Similar sentences to: \\n', sl[1], '\\n')\n",
        "temp = []\n",
        "for i in range(len(sl)):\n",
        "    temp.append(cosine(mat[i], mat[1]))\n",
        "temp = torch.from_numpy(np.array(temp))\n",
        "\n",
        "values, key = torch.topk(temp, k=6, axis=-1)\n",
        "for i in range(5):\n",
        "  print(sl[key[i+1]], '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "080203ed",
      "metadata": {
        "id": "080203ed",
        "outputId": "579425ce-9a32-4360-8358-ae10e8135a63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar sentences to: \n",
            " the patient , diagnosed with coronavirus infection and treated at home is admitted to covid dedicated multidisciplinary hospital , where surgical care is provided . \n",
            "\n",
            "in case of covid suspicion , patient were sent to dedicated covid - unit at the emergency department . \n",
            "\n",
            "first , for patient with laboratory - confi rmed infection who required hospital admission for medical reason , we examined the risk of death , admission to icu , and mechanical ventilation . \n",
            "\n",
            "patient were advised very early to stay at home , avoid public transportation and unnecessary contact , and report every symptom before coming to the outpatient clinic . \n",
            "\n",
            ", bed assignment for patient being admitted from the emergency department who may require cohorting by their covid status . \n",
            "\n",
            "the patient should be admitted to the surgical department , where treatment is provided only to those covid negative . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Similar sentences to: \\n', sl[5], '\\n')\n",
        "temp = []\n",
        "for i in range(len(sl)):\n",
        "    temp.append(cosine(mat[i], mat[5]))\n",
        "temp = torch.from_numpy(np.array(temp))\n",
        "\n",
        "values, key = torch.topk(temp, k=6, axis=-1)\n",
        "for i in range(5):\n",
        "  print(sl[key[i+1]], '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c6599fad",
      "metadata": {
        "id": "c6599fad",
        "outputId": "3be4b488-5866-4037-e3bb-13d66422631c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar sentences to: \n",
            " feline infectious peritonitis . \n",
            "\n",
            "pneumoniae infection . \n",
            "\n",
            "reconditum infection . \n",
            "\n",
            "influenzae and atypical pathogen . \n",
            "\n",
            "influenzae and atypical pathogen . \n",
            "\n",
            "multiple organ dysfunction syndrome mod and shock were defined following international criterion . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Similar sentences to: \\n', sl[17], '\\n')\n",
        "temp = []\n",
        "for i in range(len(sl)):\n",
        "    temp.append(cosine(mat[i], mat[17]))\n",
        "temp = torch.from_numpy(np.array(temp))\n",
        "\n",
        "values, key = torch.topk(temp, k=6, axis=-1)\n",
        "for i in range(5):\n",
        "  print(sl[key[i+1]], '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a0eb6938",
      "metadata": {
        "id": "a0eb6938",
        "outputId": "bd48bed3-86f4-405c-9569-889e12a8a145",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar sentences to: \n",
            " in addition , we also observed that higher disease severity wa associated with more elevated plasma ifn - , il - 1b , il il ip and mcp concentration . \n",
            "\n",
            "they concluded that the local cardiac production of inflammatory cytokine containing il - 1β , il and il were elevated in patient with ac . \n",
            "\n",
            "all the other measured cytokine and chemokines il , il il il - 12p70 , tnf - , ifn - , rantes and mig did not show any significant difference before and after corticosteroid treatment . \n",
            "\n",
            "coagulation abnormality is also contained in low platelet count , increased fibrinogen , - dimer level and prolonged pt in severe patient . \n",
            "\n",
            "substantial difference were found among strain in the capacity to induce interleukin il and tumour necrosis factor tnf - , while the difference for il and il were le pronounced . \n",
            "\n",
            "alt , ast , tbil , bun , and cr level showed the significant increase , following decrease in albumin level a the main liver and kidney outcome in severe patient compared to non - severe one . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Similar sentences to: \\n', sl[213], '\\n')\n",
        "temp = []\n",
        "for i in range(len(sl)):\n",
        "    temp.append(cosine(mat[i], mat[213]))\n",
        "temp = torch.from_numpy(np.array(temp))\n",
        "\n",
        "values, key = torch.topk(temp, k=6, axis=-1)\n",
        "for i in range(5):\n",
        "  print(sl[key[i+1]], '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "-ri-dHXNgv3y",
      "metadata": {
        "id": "-ri-dHXNgv3y",
        "outputId": "d773aed0-0186-4862-b48d-4d4ade8a5f7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar sentences to: \n",
            " vivax infection in south indian city of mangaluru merozoite surface protein msp malaria still remains a serious health problem in several tropical region . \n",
            "\n",
            "glabrata in northern europe ; in the latter , acquired echinocandin resistance is emerging . \n",
            "\n",
            "climate change is likely to accelerate pandemic emergence because temperate zone encompass larger area of the globe , expanding vector territory and favoring bacterial pathogen . \n",
            "\n",
            "ibv is endemic in most country around the world and cause huge economical loss in the poultry industry . \n",
            "\n",
            "the map suggest that there are potential hotspot of disease emergence - particularly in central america , tropical africa and south asia - that warrant greater surveillance . \n",
            "\n",
            "infectious bronchitis virus ibv cause highly contagious disease in chicken and is one of the major problem of poultry industry in many country . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Similar sentences to: \\n', sl[2612], '\\n')\n",
        "temp= []\n",
        "for i in range(len(sl)):\n",
        "    temp.append(cosine(mat[i], mat[2612]))\n",
        "temp = torch.from_numpy(np.array(temp))\n",
        "\n",
        "values, key = torch.topk(temp, k=6, axis=-1)\n",
        "for i in range(5):\n",
        "  print(sl[key[i+1]], '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8d34be6",
      "metadata": {
        "id": "e8d34be6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_03_ShankarRamVasudevan_MCS202114_(1) (1).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "285e44766fef620a5325f4ccc340373809d9df413359230dde72cf19e88cda48"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}